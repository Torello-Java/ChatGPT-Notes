<!DOCTYPE HTML>
<HTML>
<HEAD>
<TITLE>SoftMax — Turning Scores Into Weights</TITLE>
<META CHARSET="utf-8" />
<LINK REL="stylesheet" TYPE="text/css" HREF="styles.css" />
</HEAD>
<BODY>

<H1>SOFTMAX</H1>
<BR /><BR />

<B>WHAT IT DOES</B><BR />
Turns per-token scores into a probability distribution over positions. Each row sums to 1,
values are non-negative, and large scores dominate.<BR />
<BR />

<B>WHAT ARE "LOGITS"?</B><BR />
"Logits" are raw, unnormalized scores output by a model before any normalization (like SoftMax).<BR />
They can be positive or negative and are not probabilities until SoftMax is applied.<BR />
<BR />

<B>DEFINITION (ROW-WISE)</B><BR />
<DIV CLASS="SNIP MATH">
Given a row s of length L:<BR />
softmax(s)<SUB>i</SUB> = exp(s<SUB>i</SUB> - max(s)) / Σ<SUB>j</SUB> exp(s<SUB>j</SUB> - max(s))<BR />
<BR />
Subtracting max(s) improves numerical stability.
</DIV>
<BR />

<B>WHERE IT SITS</B><BR />
After scoring S = (Q K<SUP>T</SUP>) / √d_k and after adding any masks,
apply SoftMax across the key/position dimension for each head.<BR />
<BR />

<B>TEMPERATURE (OPTIONAL)</B><BR />
<DIV CLASS="SNIP">
softmax(s / T) with T&gt;1 flattens; T&lt;1 sharpens.
</DIV>
<BR />

<B>MASKING</B><BR />
Add a very negative number (effectively −∞) to disallowed positions (padding or future tokens)
before SoftMax so they receive probability ~0.<BR />
<BR />

<B>GAUSSIAN CONNECTION?</B><BR />
SoftMax is NOT a Gaussian. It uses exponentials and normalization to map logits to a simplex.<BR />
However, if logits have a quadratic form (e.g., negative squared distance), the resulting
distribution can resemble a Gaussian kernel over positions. In standard attention,
the Q·K<SUP>T</SUP> similarity is linear, not quadratic; Gaussian behavior is not guaranteed.<BR />

</BODY>
</HTML>
