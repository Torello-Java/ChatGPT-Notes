<!DOCTYPE HTML>
<HTML>
<HEAD>
<TITLE>Residual + LayerNorm — Add & Normalize</TITLE>
<META CHARSET="utf-8" />
<LINK REL="stylesheet" TYPE="text/css" HREF="styles.css" />
</HEAD>
<BODY>

<H1>RESIDUAL + LAYERNORM</H1>
<BR /><BR />

<B>RESIDUAL</B><BR />
Add the sublayer output back to its input to preserve signal and help optimization.<BR />
<DIV CLASS="SNIP">
Given input X:[L, d_model] and sublayer F(·):
Y = X + F(X)
</DIV>
<BR />

<B>LAYERNORM (NOT "SUM TO ONE")</B><BR />
LayerNorm normalizes each token's feature vector to zero mean and unit variance, then applies a
learned scale (gamma) and shift (beta). It does NOT make components sum to 1.<BR />
<BR />
<DIV CLASS="SNIP MATH">
For a token vector y ∈ ℝ^{d_model}:<BR />
μ = mean(y),  σ = sqrt( var(y) + ε )<BR />
LN(y) = γ * ( (y - μ) / σ ) + β
</DIV>
<BR />

<B>PRE-LN VS POST-LN</B><BR />
<UL CLASS="JDUL">
  <LI>POST-LN (original): Y = LN( X + F(X) )</LI>
  <LI>PRE-LN (modern):    Y = X + F( LN(X) )   — generally more stable for deep stacks.</LI>
</UL>
<BR />

<B>WHY IT HELPS</B><BR />
<UL CLASS="JDUL">
  <LI>Residuals give an identity path for gradients.</LI>
  <LI>LayerNorm stabilizes scale and reduces covariate shift layer-to-layer.</LI>
</UL>

</BODY>
</HTML>
